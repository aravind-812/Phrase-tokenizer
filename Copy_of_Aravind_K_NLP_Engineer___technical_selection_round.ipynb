{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j1iQo5365SN"
      },
      "source": [
        ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0OYDAB7XTCW"
      },
      "source": [
        "#TEXTIFY AI NLP ENGINEER_TECHNICAL SELECTION ROUND\n",
        "\n",
        "#HELLO EVERYONE\n",
        "\n",
        "#Thank you for taking your time to work on these problem sets.\n",
        "\n",
        "#The aim of this problem set is to evaluate your skills in the following domains:\n",
        "#     -Python programming basics (list comprehension, iterations, looping, if-else statements, etc )\n",
        "#     -Web scrapping libraries (BeautifulSoup, requests)\n",
        "#     -Data structures library (Pandas)\n",
        "#     -NLP libraries (SpaCy, NLTK)\n",
        "#     -ML/DL library (scikit-learn or TensorFlow)\n",
        "#     -Word embedding models (word2vec or GloVe)\n",
        "\n",
        "#Please follow step-by-step process as mentioned below, and submit your solutions before 20th July.\n",
        "\n",
        "#Problem set difficulty -> Intermediate     ; Time Requirement-> 4 to 6 hours (experienced developer), 2 to 3 days (Learners)\n",
        "\n",
        "#In case of any difficulty in accessing the required files, please contact the team at textifyai@gmail.com\n",
        "\n",
        "#Applicant's name -> ________Aravind K______________________\n",
        "#Email : aravindkamarsu32@gmail.com"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqKy9jp1BmVv"
      },
      "source": [
        "**STEP 1**- Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57r38-36Bf-P"
      },
      "source": [
        "#-Download the dataset (Essay_samples.csv) from your email.\n",
        "#-Copy this notebook into your own google Colab account.\n",
        "#-Upload the dataset on the notebook as below\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/Essay_samples - Sheet1.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4xtkR_-6Hjcv",
        "outputId": "71db7f39-98e5-4de2-b95b-5717e4e528d0"
      },
      "source": [
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Essay Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>THE ALARM CLOCK IS, TO MANY high school studen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I HAVE ALWAYS BEEN A MATH-SCIENCE girl. I sigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WHEN I WAS FOUR YEARS OLD, I fell in love. It ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THIS SUMMER, I WENT TO THE governor’s Honors P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>THIS PAST SUMMER I HAD THE opportunity to part...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>CRIME SCENE REPORT\\nCrime: Missing Person\\nLoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>AFTER SPENDING A WEEK WITH JOHN NASH, I may ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>I GREW UP IN A FOUR-ROOM apartment in the midd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>AS I SAT AT A TABLE in the corner of a cafe, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>NOVEMBER 23, TWO YEARS AGO: THANKSGIVING. My m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Essay Text\n",
              "0   THE ALARM CLOCK IS, TO MANY high school studen...\n",
              "1   I HAVE ALWAYS BEEN A MATH-SCIENCE girl. I sigh...\n",
              "2   WHEN I WAS FOUR YEARS OLD, I fell in love. It ...\n",
              "3   THIS SUMMER, I WENT TO THE governor’s Honors P...\n",
              "4   THIS PAST SUMMER I HAD THE opportunity to part...\n",
              "..                                                ...\n",
              "94  CRIME SCENE REPORT\\nCrime: Missing Person\\nLoc...\n",
              "95  AFTER SPENDING A WEEK WITH JOHN NASH, I may ha...\n",
              "96  I GREW UP IN A FOUR-ROOM apartment in the midd...\n",
              "97  AS I SAT AT A TABLE in the corner of a cafe, h...\n",
              "98  NOVEMBER 23, TWO YEARS AGO: THANKSGIVING. My m...\n",
              "\n",
              "[99 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJaD7z-JYve8"
      },
      "source": [
        "**STEP 2** - Web Scrapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqgEXWRxYzfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36053643-58d9-4831-9690-f2db37f7eecd"
      },
      "source": [
        "#-Use BeautifulSoup library to scrape 50 articles from (www.bbc.com).\n",
        "#-We will then use the scrapped articles to create a hybrid database.\n",
        "\n",
        "! pip install beautifulsoup4\n",
        "! pip install requests\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# url = 'https://www.bbc.com/'\n",
        "# response = requests.get(url)\n",
        "# data = response.text\n",
        "# soup = BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "#Your web-scraping code goes here\n",
        "def extract_links_from_articles():\n",
        "    bccTechNews = requests.get('https://www.bbc.com/news/technology')\n",
        "    TechSoup = bs(bccTechNews.content, 'html.parser')\n",
        "    links = []\n",
        "    for a in TechSoup.find_all('a', href=True):\n",
        "      if '/news/technology-' in a['href'] and '#comp-comments-button' not in a['href']: \n",
        "        link = 'https://www.bbc.com' + a['href']\n",
        "        links.append(link)\n",
        "    #\n",
        "    bccAsiaNews = requests.get('https://www.bbc.com/news/world/asia')\n",
        "    asiaSoup = bs(bccAsiaNews.content, 'html.parser')\n",
        "    for a in asiaSoup.find_all('a', href=True):\n",
        "      if 'news/world-asia-' in a['href']: \n",
        "        link = 'https://www.bbc.com' + a['href']\n",
        "        links.append(link)\n",
        "\n",
        "    bccUkNews = requests.get('https://www.bbc.com/news/uk')\n",
        "    ukSoup = bs(bccUkNews.content, 'html.parser')\n",
        "    for a in ukSoup.find_all('a', href=True):\n",
        "         \n",
        "      if 'https://www.bbc.com' in a['href']:\n",
        "        link = a['href']\n",
        "        links.append(link)\n",
        "      if 'news/uk-' in a['href']  and 'https://www.bbc' not in a['href']: \n",
        "        link = 'https://www.bbc.com' + a['href']\n",
        "        links.append(link)  \n",
        "    return links\n",
        "\n",
        "def scrape_single_article(url:str):\n",
        "    bccNews = requests.get(url)\n",
        "    soup = bs(bccNews.content, 'html.parser')\n",
        "    bbcDf = pd.DataFrame(columns=['Essay Text'])\n",
        "    articles = soup.find_all('div', class_='ssrcss-18snukc-RichTextContainer e5tfeyi1')\n",
        "    text = ''\n",
        "    for article in articles:\n",
        "        try:\n",
        "            paragraph = article.find('p').getText()\n",
        "            text = text + paragraph\n",
        "        except:    \n",
        "            pass\n",
        "    return text \n",
        "\n",
        "def scrape_multiple_articles():\n",
        "    links = extract_links_from_articles()\n",
        "    links = set(links) \n",
        "    bbcDf = pd.DataFrame(columns=['Essay Text'])\n",
        "    for i in links:\n",
        "      articleContent = scrape_single_article(i)\n",
        "      if len(articleContent.split(\" \")) >= 40:\n",
        "        bbcDf = bbcDf.append({'Essay Text': articleContent}, ignore_index=True)\n",
        "    return bbcDf    \n",
        "\n",
        "df1 = scrape_multiple_articles()\n",
        "\n",
        "#\n",
        "#\n",
        "\n",
        "#-Create a new pandas table and add the essay database and the scrapped articles.\n",
        "#df_new = df_essay + df_scraped_articles\n",
        "\n",
        "df = pd.concat([df, df1], axis=0,join='outer',ignore_index=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cKOWMHhPKTQS",
        "outputId": "0dd872e2-8e0c-4fb7-9e35-c411e266e43e"
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Essay Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>THE ALARM CLOCK IS, TO MANY high school studen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I HAVE ALWAYS BEEN A MATH-SCIENCE girl. I sigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WHEN I WAS FOUR YEARS OLD, I fell in love. It ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THIS SUMMER, I WENT TO THE governor’s Honors P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>THIS PAST SUMMER I HAD THE opportunity to part...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>Police have said they \"ran out\" of fixed penal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Some children and young people aged over 12 in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>More than 50 beachgoers had to be rescued when...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>Both the UK government and the EU must re-nego...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>Restrictions to our daily lives are being rela...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Essay Text\n",
              "0    THE ALARM CLOCK IS, TO MANY high school studen...\n",
              "1    I HAVE ALWAYS BEEN A MATH-SCIENCE girl. I sigh...\n",
              "2    WHEN I WAS FOUR YEARS OLD, I fell in love. It ...\n",
              "3    THIS SUMMER, I WENT TO THE governor’s Honors P...\n",
              "4    THIS PAST SUMMER I HAD THE opportunity to part...\n",
              "..                                                 ...\n",
              "143  Police have said they \"ran out\" of fixed penal...\n",
              "144  Some children and young people aged over 12 in...\n",
              "145  More than 50 beachgoers had to be rescued when...\n",
              "146  Both the UK government and the EU must re-nego...\n",
              "147  Restrictions to our daily lives are being rela...\n",
              "\n",
              "[148 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ4Si0r7eRzn"
      },
      "source": [
        "**STEP 3** - Spacy dependency parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK5KwYTEeWMB"
      },
      "source": [
        "#import Spacy library\n",
        "\n",
        "import spacy\n",
        "from __future__ import unicode_literals, print_function\n",
        "from spacy.lang.en import English # updated\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "#In case you are unfamiliar with dependency parsing, review the document attached with the mail.\n",
        "\n",
        "#Use .split(\"\\n\") to split the essays into paragraphs.\n",
        "#Use SpaCy to split the paragraphs into sentences.\n",
        "#Use SpaCy dependency parsing feature to extract phrases from the sentences \n",
        "\n",
        "##HINT-> USE THE SUBTREE argument of TOKEN TO EXTRACT THE COMPLETE PHRASE \n",
        "#EXAMPLE:\n",
        "#if token == 'nsubj':\n",
        "#  subtree=token.subtree\n",
        "#  break\n",
        "#print([(t.text) for t in subtree])\n",
        "\n",
        "\n",
        "#-Create a sequential list of phrases as they appear in the text.\n",
        "\n",
        "def phrase_converter(sentence):\n",
        "  piano_doc = nlp(sentence)\n",
        "  listo = []\n",
        "\n",
        "  for token in piano_doc:\n",
        "      verbFlag = 0\n",
        "      subjFlag = 0\n",
        "      if token.dep_ not in ['ROOT','xcomp','ccomp','advcl','nsubjpass','acl','conj','nsubj','cconj']: #if the following elements are present in the words then it's not considered to be a phrase\n",
        "        subtree=token.subtree\n",
        "        subtreeList = [(t.text) for t in subtree]\n",
        "        subtreeWords = \" \".join(str(x) for x in subtreeList)\n",
        "        phraseCheck = nlp(subtreeWords)\n",
        "        for token in phraseCheck:\n",
        "          if token.dep_ ==  'nsubj':\n",
        "            subjFlag == 1\n",
        "          if token.pos_ == 'VERB':\n",
        "            verbFlag == 1\n",
        "        if verbFlag == 1 or subjFlag == 1:\n",
        "          #print(\"passed sentence\", subtreeList)\n",
        "          pass   \n",
        "        elif len(subtreeList)>4:\n",
        "          #print(\"passed sentence\", subtreeList)\n",
        "          pass\n",
        "        else:\n",
        "          listo.append(\" \".join(subtreeList))\n",
        "        #print(token, token.dep_ ,subtreeList)\n",
        "      else:\n",
        "        listo.append(str(token))\n",
        "        #print(token)\n",
        "  right = len(listo)-1\n",
        "  left = right-1\n",
        "  while left<=right and left>-1:\n",
        "    if listo[left] in listo[right]:\n",
        "      if len(listo[left]) < len(listo[right]):\n",
        "        listo[left] = \"\"\n",
        "    else:\n",
        "      right=left\n",
        "    left = left-1\n",
        "\n",
        "\n",
        "  right = left+1\n",
        "  left = 0\n",
        "  while left<=right and right<len(listo):\n",
        "    if listo[right] in listo[left]:\n",
        "      if len(listo[right]) < len(listo[left]):\n",
        "        listo[right] = \"\"\n",
        "    else:\n",
        "      left=right\n",
        "    right = right+1\n",
        "  listo = [ele for ele in listo if ele != ''] \n",
        "  return listo\n",
        "\n",
        "\n",
        "nlp1 = English()\n",
        "nlp1.add_pipe(nlp1.create_pipe('sentencizer')) # updated\n",
        "essay_text = df['Essay Text'].tolist()\n",
        "sentences = [sent.string.strip() for sent in (nlp1(essay_text[0].split(\"\\n\")[0])).sents]\n",
        "main_list = []\n",
        "main_list=phrase_converter(sentences[0])\n",
        "for i in range(len(essay_text)):\n",
        "  x = essay_text[i].split(\"\\n\")\n",
        "  for j in range(len(x)):\n",
        "    doc = nlp1(x[j])\n",
        "    sentences = [sent.string.strip() for sent in doc.sents]\n",
        "    for k in range(len(sentences)):\n",
        "      piano_text = str(sentences[k])\n",
        "      main_list.extend(phrase_converter(piano_text))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMbW24z4f2ii"
      },
      "source": [
        "**STEP 4** - N-gram next phrase prediction model using either ML/DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlPm1zTTf417"
      },
      "source": [
        "#Implement N-gram next phrase prediction model.\n",
        "\n",
        "#Please find more information about the next word prediction model here-> \n",
        "#https://medium.com/analytics-vidhya/build-a-simple-predictive-keyboard-using-python-and-keras-b78d3c88cffb\n",
        "#Also, here ->  https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf\n",
        "\n",
        "#NOTE : Instead of using WORDS, we are using PHRASES (as you might have extracted in the previous step) in this project.\n",
        "\n",
        "#Play around with the model to improve accuary if possible."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stXTsCoC2wVH"
      },
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq\n",
        "words = main_list\n",
        "unique_words = np.unique(words)\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))\n",
        "# unique_word_index['satare'] = len(unique_words)+1\n",
        "# list(unique_word_index)[-1]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab0YjL3jxDM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961cc3c5-36c1-416e-912b-f81fa78280c1"
      },
      "source": [
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(words) - WORD_LENGTH):\n",
        "    prev_words.append(words[i:i + WORD_LENGTH])\n",
        "    next_words.append(words[i + WORD_LENGTH])\n",
        "\n",
        "print(prev_words[0])\n",
        "print(next_words[0])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['THE', 'ALARM', 'CLOCK', 'IS', ',']\n",
            "MANY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVLITCTb2wlB",
        "outputId": "e71f1124-944c-4e47-8409-c6557ccd5820"
      },
      "source": [
        "X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
        "Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)\n",
        "\n",
        "for i, each_words in enumerate(prev_words):\n",
        "    for j, each_word in enumerate(each_words):\n",
        "        X[i, j, unique_word_index[each_word]] = 1\n",
        "    Y[i, unique_word_index[next_words[i]]] = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))\n",
        "model.add(Dense(len(unique_words)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(X, Y, validation_split=0.10, batch_size=128, epochs=30, shuffle=False).history        "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "499/499 [==============================] - 32s 24ms/step - loss: 7.2833 - accuracy: 0.1011 - val_loss: 7.0559 - val_accuracy: 0.1225\n",
            "Epoch 2/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 6.3271 - accuracy: 0.1557 - val_loss: 7.5152 - val_accuracy: 0.1258\n",
            "Epoch 3/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 5.9616 - accuracy: 0.1788 - val_loss: 8.2518 - val_accuracy: 0.1274\n",
            "Epoch 4/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 6.0258 - accuracy: 0.1890 - val_loss: 8.3947 - val_accuracy: 0.0978\n",
            "Epoch 5/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 6.2005 - accuracy: 0.2077 - val_loss: 9.1010 - val_accuracy: 0.0873\n",
            "Epoch 6/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 6.1543 - accuracy: 0.2277 - val_loss: 8.9206 - val_accuracy: 0.0925\n",
            "Epoch 7/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 5.9684 - accuracy: 0.2622 - val_loss: 8.8768 - val_accuracy: 0.0785\n",
            "Epoch 8/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 5.7582 - accuracy: 0.3039 - val_loss: 8.7170 - val_accuracy: 0.0939\n",
            "Epoch 9/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 5.5335 - accuracy: 0.3389 - val_loss: 9.1018 - val_accuracy: 0.0464\n",
            "Epoch 10/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 5.3359 - accuracy: 0.3750 - val_loss: 9.2087 - val_accuracy: 0.0801\n",
            "Epoch 11/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 5.1286 - accuracy: 0.4077 - val_loss: 8.9316 - val_accuracy: 0.0594\n",
            "Epoch 12/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.9930 - accuracy: 0.4290 - val_loss: 9.1420 - val_accuracy: 0.0802\n",
            "Epoch 13/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.8703 - accuracy: 0.4524 - val_loss: 9.0606 - val_accuracy: 0.0897\n",
            "Epoch 14/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.7808 - accuracy: 0.4690 - val_loss: 9.2336 - val_accuracy: 0.0637\n",
            "Epoch 15/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.7223 - accuracy: 0.4816 - val_loss: 9.2915 - val_accuracy: 0.0938\n",
            "Epoch 16/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.6818 - accuracy: 0.4947 - val_loss: 9.6739 - val_accuracy: 0.0498\n",
            "Epoch 17/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.6217 - accuracy: 0.5041 - val_loss: 9.4318 - val_accuracy: 0.0860\n",
            "Epoch 18/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.5603 - accuracy: 0.5160 - val_loss: 9.4494 - val_accuracy: 0.0605\n",
            "Epoch 19/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.5238 - accuracy: 0.5226 - val_loss: 9.7039 - val_accuracy: 0.0647\n",
            "Epoch 20/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.4886 - accuracy: 0.5269 - val_loss: 9.6833 - val_accuracy: 0.0923\n",
            "Epoch 21/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.4485 - accuracy: 0.5303 - val_loss: 9.3807 - val_accuracy: 0.0739\n",
            "Epoch 22/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.4190 - accuracy: 0.5370 - val_loss: 9.3993 - val_accuracy: 0.0839\n",
            "Epoch 23/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.3951 - accuracy: 0.5424 - val_loss: 9.5517 - val_accuracy: 0.0672\n",
            "Epoch 24/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.3945 - accuracy: 0.5445 - val_loss: 9.5599 - val_accuracy: 0.0877\n",
            "Epoch 25/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.3926 - accuracy: 0.5513 - val_loss: 9.5851 - val_accuracy: 0.0805\n",
            "Epoch 26/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.3598 - accuracy: 0.5536 - val_loss: 9.4651 - val_accuracy: 0.0780\n",
            "Epoch 27/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.3062 - accuracy: 0.5548 - val_loss: 9.2412 - val_accuracy: 0.0849\n",
            "Epoch 28/30\n",
            "499/499 [==============================] - 11s 21ms/step - loss: 4.3107 - accuracy: 0.5604 - val_loss: 9.3336 - val_accuracy: 0.0856\n",
            "Epoch 29/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.3053 - accuracy: 0.5626 - val_loss: 9.3766 - val_accuracy: 0.0825\n",
            "Epoch 30/30\n",
            "499/499 [==============================] - 11s 22ms/step - loss: 4.2967 - accuracy: 0.5650 - val_loss: 9.5068 - val_accuracy: 0.0825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpsZ-Em55-Gc"
      },
      "source": [
        "model.save('keras_next_phrase_model.h5')\n",
        "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
        "model = load_model('keras_next_phrase_model.h5')\n",
        "history = pickle.load(open(\"history.p\", \"rb\"))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3XUaWI96TML",
        "outputId": "839d11cf-ce4d-4e95-f900-389f0e1ee86d"
      },
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, WORD_LENGTH, len(unique_words)))\n",
        "    for t, word in enumerate(text.split()):\n",
        "        x[0, t, unique_word_index[word]] = 1\n",
        "    return x\n",
        "prepare_input(\"It is not a lack\".lower())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prz5q55i6TPe",
        "outputId": "549914e5-64e6-4b42-9b8f-43902b187f07"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)\n",
        "\n",
        "def predict_completions(text, n=3):\n",
        "    if text == \"\":\n",
        "        return(\"0\")\n",
        "    x = prepare_input(text)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [unique_words[idx] for idx in next_indices]\n",
        "\n",
        "q =  \"he is doing it \" \n",
        "\n",
        "\n",
        "seq = \" \".join(tokenizer.tokenize(q.lower())[0:5])\n",
        "#print(tokenizer.tokenize(q.lower())[0:5])\n",
        "print(\"next possible words: \", predict_completions(seq, 5))  "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "next possible words:  ['\"', 'and', ',', 'the', 'for']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx4qh09_gN3B"
      },
      "source": [
        "**STEP 5**- Fine tuning word embedding (GloVe or Word2Vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErpvoXs9gQiW"
      },
      "source": [
        "#Fine tune word embedding model using the 'Noun' phrases in the data.\n",
        "#In this step, we will fine tune the word embedding model (either GloVe or Word2Vec) on the Nouns (use POS tag from spacy) from our hybrid database.\n",
        "#First, extract the nouns in the database in a sequential manner.\n",
        "#Second, train the word embedding model on the sequential list.\n",
        "#NOTE: This step requires extracting words (not phrases, as we did before). Please do this step separately from the above step.\n",
        "import spacy\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "import json\n",
        "import pandas as pd\n",
        "import string\n",
        "import time\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "from gensim.models import Word2Vec\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "noun_list = []\n",
        "for i in range(len(main_list)):\n",
        "  doc = nlp(main_list[i])\n",
        "  for chunk in doc.noun_chunks:\n",
        "    if str(chunk) not in noun_list:\n",
        "      noun_list.append(str(chunk))\n",
        "wv = api.load('word2vec-google-news-300')      \n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "    Callback to print loss after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        elif self.epoch % 100 == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss\n",
        "w2v_model = Word2Vec(size = 300,\n",
        "                     window = 15,\n",
        "                     min_count = 2,\n",
        "                     workers = 20,\n",
        "                     sg = 1,\n",
        "                     negative = 5,\n",
        "                     sample = 1e-5)\n",
        "w2v_model.build_vocab(myFinallist)\n",
        "start = time.time()\n",
        "w2v_model.train(myFinallist, \n",
        "                total_examples=w2v_model.corpus_count, \n",
        "                epochs=100, \n",
        "                report_delay=1,\n",
        "                compute_loss = True,\n",
        "                callbacks=[callback()]) \n",
        "end = time.time()\n",
        "print(\"elapsedtime in seconds :\"+ str(end - start))\n",
        "w2v_model.save('word2vec.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYVZqmcGggXi"
      },
      "source": [
        "**BONUS STEP**\n",
        "Use the word-embedding model to promote/demote suggestions from the N-gram model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Z3LXoNY_uh"
      },
      "source": [
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-m0i19gxI2"
      },
      "source": [
        ""
      ]
    }
  ]
}
